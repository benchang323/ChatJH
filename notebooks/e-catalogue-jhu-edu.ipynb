{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(start, success_file_path, error_file_path):\n",
    "    unseen = set([start])\n",
    "    seen = set([])\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        while unseen:\n",
    "            url = unseen.pop()\n",
    "            seen.add(url)\n",
    "            executor.submit(crawl_website, url, seen, unseen, success_file_path, error_file_path)\n",
    "\n",
    "    print(f\"Finished crawling {start}\")\n",
    "    return seen \n",
    "\n",
    "def crawl_website(url, seen, unseen, success_file_path, error_file_path):\n",
    "    try:\n",
    "        new_links = getNewLinks(url, seen, unseen, success_file_path)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            for link in new_links:\n",
    "                print(f\"Adding: {link}\")\n",
    "                unseen.add(link)  \n",
    "                executor.submit(crawl_website, link, seen, unseen, success_file_path, error_file_path)\n",
    "    except:\n",
    "        print(f\"Error crawling {url} due to {sys.exc_info()[0]}\")\n",
    "        with open(error_file_path, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "def save_links_to_file(links, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        for link in links:\n",
    "            if not is_duplicate_link(link, file_path):\n",
    "                f.write(f\"{link}\\n\")\n",
    "                \n",
    "def getNewLinks(url, seen, unseen, success_file_path):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [link['href'] for link in soup.find_all('a') if link.has_attr('href') and not any(ext in link['href'] for ext in ['.png', '.pdf', '.jpg', '.jpeg', '~json/', 'javascript:;', 'mailto:', 'webcal:'])]\n",
    "        links = [urljoin(url, link) for link in links]\n",
    "        links = [link for link in links if link not in seen and link not in unseen and urlparse(link).netloc.endswith('e-catalogue.jhu.edu')] # Change this to limit domain\n",
    "        save_links_to_file(links, success_file_path)\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Error getting {url} with status code {response.status_code} and reason {response.reason}\")\n",
    "        print(f\"Error full response: {response.text}\")\n",
    "    return []\n",
    "\n",
    "def is_duplicate_link(link, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if link.strip() == line.strip():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled_links = crawl_web('https://e-catalogue.jhu.edu/', \"../data/e-catalogue-jhu-edu/success_links.txt\", \"../data/e-catalogue-jhu-edu/error_links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/33bb8_h50z75_0klrf43_bqm0000gn/T/ipykernel_21218/1015152670.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"../backend/data/e-catalogue-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://e-catalogue.jhu.edu//#contentarea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://e-catalogue.jhu.edu/azindex/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://e-catalogue.jhu.edu/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://e-catalogue.jhu.edu/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://e-catalogue.jhu.edu/university-wide-po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13394</th>\n",
       "      <td>https://e-catalogue.jhu.edu/search/?P=AS.020.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13395</th>\n",
       "      <td>https://e-catalogue.jhu.edu/search/?P=AS.020.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13398</th>\n",
       "      <td>https://e-catalogue.jhu.edu/archive/2015-16/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13399</th>\n",
       "      <td>https://e-catalogue.jhu.edu/archive/2015-16/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13400</th>\n",
       "      <td>https://e-catalogue.jhu.edu/search/?P=AS.070.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11761 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Link\n",
       "0              https://e-catalogue.jhu.edu//#contentarea\n",
       "1                   https://e-catalogue.jhu.edu/azindex/\n",
       "2                           https://e-catalogue.jhu.edu/\n",
       "3                           https://e-catalogue.jhu.edu/\n",
       "4      https://e-catalogue.jhu.edu/university-wide-po...\n",
       "...                                                  ...\n",
       "13394   https://e-catalogue.jhu.edu/search/?P=AS.020.346\n",
       "13395   https://e-catalogue.jhu.edu/search/?P=AS.020.347\n",
       "13398  https://e-catalogue.jhu.edu/archive/2015-16/de...\n",
       "13399  https://e-catalogue.jhu.edu/archive/2015-16/de...\n",
       "13400  https://e-catalogue.jhu.edu/search/?P=AS.070.7...\n",
       "\n",
       "[11761 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load crawled links from file into a dataframe (in case of termination)\n",
    "df = pd.read_csv(\"../backend/data/e-catalogue-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Rename column\n",
    "df.rename(columns={0: \"Link\"}, inplace=True)\n",
    "\n",
    "# Normalize dataframe (remove any links that are not jhu.edu in base url or no https:// in the link)\n",
    "df = df[df['Link'].str.contains(\"https://e-catalogue.jhu.edu\")]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove leading/trailing whitespace, condense multiple whitespaces to single\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def scrape_page(url, q):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'li', 'div', 'table', 'th', 'td', 'tr', 'ol', 'ul', 'blockquote', 'pre', 'code', 'caption', 'dt', 'dd']\n",
    "\n",
    "    text_data = []\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            text = clean_text(element.get_text())\n",
    "            if text:\n",
    "                text_data.append(text)\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(text_data) - 1):\n",
    "        json_data.append({\n",
    "            'prompt': text_data[i],\n",
    "            'completion': text_data[i + 1]\n",
    "        })\n",
    "        \n",
    "    # add to queue\n",
    "    q.put(json_data)\n",
    "    \n",
    "def write_to_file(q):\n",
    "    with open('../backend/data/e-catalogue-jhu-edu/data.json', 'a') as f:\n",
    "        while True:\n",
    "            data = q.get()\n",
    "            if data is None:\n",
    "                break\n",
    "            json.dump(data, f)\n",
    "            f.write('\\n')\n",
    "            q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of URLs to scrape\n",
    "# urls = df['Link'].tolist()\n",
    "\n",
    "# # create a queue\n",
    "# q = queue.Queue()\n",
    "\n",
    "# # create a separate thread to write data to file\n",
    "# file_writer = threading.Thread(target=write_to_file, args=(q,))\n",
    "# file_writer.start()\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#     futures = {executor.submit(scrape_page, url, q) for url in urls}\n",
    "#     concurrent.futures.wait(futures)\n",
    "\n",
    "# # stop the file writing thread\n",
    "# q.put(None)\n",
    "# file_writer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prompt/Completion pairs before cleaning: 21429394\n",
      "Number of words scraped (prompt): 903011934\n",
      "Number of words scraped (completion): 902721946\n",
      "Number of Prompt/Completion pairs after cleaning: 186804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Search Courses Keyword Academic Year Term 2022...</td>\n",
       "      <td>Search Courses Keyword Academic Year Term 2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Keyword Academic Year Term 2022-23 2020-21 202...</td>\n",
       "      <td>Keyword Academic Year Term 2022-23 2020-21 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Keyword Academic Year Term 2022-23 2020-21 202...</td>\n",
       "      <td>Keyword Academic Year Term 2022-23 2020-21 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ADVANCED SEARCH Subject Any Subject Applied Ec...</td>\n",
       "      <td>Subject Any Subject Applied Economics - AS.440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Subject Any Subject Applied Economics - AS.440...</td>\n",
       "      <td>Welcome to Course Search Use the search panel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21429387</th>\n",
       "      <td>Teaching Writing, Certificate</td>\n",
       "      <td>AS.010 (History of Art) AS.020 (Biology) AS.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21429388</th>\n",
       "      <td>AS.010 (History of Art) AS.020 (Biology) AS.03...</td>\n",
       "      <td>/â€‹course-â€‹search/â€‹api/â€‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21429391</th>\n",
       "      <td>Homeâ€º/search/â€ºSearch Results</td>\n",
       "      <td>Johns Hopkins University Baltimore, MD 410-516...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21429392</th>\n",
       "      <td>Johns Hopkins University Baltimore, MD 410-516...</td>\n",
       "      <td>About Us Academics Schools &amp; Divisions Admissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21429393</th>\n",
       "      <td>About Us Academics Schools &amp; Divisions Admissi...</td>\n",
       "      <td>Send Page to Printer Print this page. Download...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186804 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     prompt  \\\n",
       "308       Search Courses Keyword Academic Year Term 2022...   \n",
       "311       Keyword Academic Year Term 2022-23 2020-21 202...   \n",
       "312       Keyword Academic Year Term 2022-23 2020-21 202...   \n",
       "319       ADVANCED SEARCH Subject Any Subject Applied Ec...   \n",
       "321       Subject Any Subject Applied Economics - AS.440...   \n",
       "...                                                     ...   \n",
       "21429387                      Teaching Writing, Certificate   \n",
       "21429388  AS.010 (History of Art) AS.020 (Biology) AS.03...   \n",
       "21429391                       Homeâ€º/search/â€ºSearch Results   \n",
       "21429392  Johns Hopkins University Baltimore, MD 410-516...   \n",
       "21429393  About Us Academics Schools & Divisions Admissi...   \n",
       "\n",
       "                                                 completion  \n",
       "308       Search Courses Keyword Academic Year Term 2022...  \n",
       "311       Keyword Academic Year Term 2022-23 2020-21 202...  \n",
       "312       Keyword Academic Year Term 2022-23 2020-21 202...  \n",
       "319       Subject Any Subject Applied Economics - AS.440...  \n",
       "321       Welcome to Course Search Use the search panel ...  \n",
       "...                                                     ...  \n",
       "21429387  AS.010 (History of Art) AS.020 (Biology) AS.03...  \n",
       "21429388                            /â€‹course-â€‹search/â€‹api/â€‹  \n",
       "21429391  Johns Hopkins University Baltimore, MD 410-516...  \n",
       "21429392  About Us Academics Schools & Divisions Admissi...  \n",
       "21429393  Send Page to Printer Print this page. Download...  \n",
       "\n",
       "[186804 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pairs into a dataframe\n",
    "with open('../backend/data/e-catalogue-jhu-edu/data.json', 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Error handling\n",
    "        try:\n",
    "            data.extend(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "pc_pairs_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs before cleaning: {len(pc_pairs_df)}\")\n",
    "print(f\"Number of words scraped (prompt): {pc_pairs_df['prompt'].str.split().str.len().sum()}\")\n",
    "print(f\"Number of words scraped (completion): {pc_pairs_df['completion'].str.split().str.len().sum()}\")\n",
    "\n",
    "# Remove any prompts/completion pairs that contains characters less than 15\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['prompt'].str.len() > 15]\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['completion'].str.len() > 15]\n",
    "\n",
    "# Remove duplicates\n",
    "pc_pairs_df.drop_duplicates(subset='prompt', keep='last', inplace=True)\n",
    "\n",
    "# Save to JSON file in the format of {\"prompt\": \"prompt text\", \"completion\": \"completion text\"}\n",
    "pc_pairs_df.to_json('../backend/data/e-catalogue-jhu-edu/prompt-completion-pairs.json', orient='records', lines=True)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs after cleaning: {len(pc_pairs_df)}\")\n",
    "\n",
    "pc_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 11761\n"
     ]
    }
   ],
   "source": [
    "# Get dataset statistics\n",
    "# Amount of links (from links.txt)\n",
    "print(f\"Number of links: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
