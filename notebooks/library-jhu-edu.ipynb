{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(start, success_file_path, error_file_path):\n",
    "    unseen = set([start])\n",
    "    seen = set([])\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        while unseen:\n",
    "            url = unseen.pop()\n",
    "            seen.add(url)\n",
    "            executor.submit(crawl_website, url, seen, unseen, success_file_path, error_file_path)\n",
    "\n",
    "    print(f\"Finished crawling {start}\")\n",
    "    return seen \n",
    "\n",
    "def crawl_website(url, seen, unseen, success_file_path, error_file_path):\n",
    "    try:\n",
    "        new_links = getNewLinks(url, seen, unseen, success_file_path)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            for link in new_links:\n",
    "                print(f\"Adding: {link}\")\n",
    "                unseen.add(link)  \n",
    "                executor.submit(crawl_website, link, seen, unseen, success_file_path, error_file_path)\n",
    "    except:\n",
    "        print(f\"Error crawling {url} due to {sys.exc_info()[0]}\")\n",
    "        with open(error_file_path, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "def save_links_to_file(links, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        for link in links:\n",
    "            if not is_duplicate_link(link, file_path):\n",
    "                f.write(f\"{link}\\n\")\n",
    "                \n",
    "def getNewLinks(url, seen, unseen, success_file_path):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [link['href'] for link in soup.find_all('a') if link.has_attr('href') and not any(ext in link['href'] for ext in ['.png', '.pdf', '.jpg', '.jpeg', '~json/', 'javascript:;', 'mailto:', 'webcal:', 'email-protection', 'catalyst'])]\n",
    "        links = [urljoin(url, link) for link in links]\n",
    "        links = [link for link in links if link not in seen and link not in unseen and urlparse(link).netloc.endswith('library.jhu.edu')] # Change this to limit domain\n",
    "        save_links_to_file(links, success_file_path)\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Error getting {url} with status code {response.status_code} and reason {response.reason}\")\n",
    "        print(f\"Error full response: {response.text}\")\n",
    "    return []\n",
    "\n",
    "def is_duplicate_link(link, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if link.strip() == line.strip():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled_links = crawl_web('https://library.jhu.edu/', \"../backend/data.nosync/library-jhu-edu/success_links.txt\", \"../backend/data.nosync/library-jhu-edu/error_links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/33bb8_h50z75_0klrf43_bqm0000gn/T/ipykernel_16458/588397982.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"../backend/data.nosync/library-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n",
      "Skipping line 24580: expected 1 fields, saw 2\n",
      "Skipping line 32875: expected 1 fields, saw 2\n",
      "Skipping line 33450: expected 1 fields, saw 2\n",
      "Skipping line 50838: expected 1 fields, saw 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://catalyst.library.jhu.edu/shibboleth_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://catalyst.library.jhu.edu/shibboleth_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://catalyst.library.jhu.edu/shibboleth_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://catalyst.library.jhu.edu/shibboleth_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://catalyst.library.jhu.edu/shibboleth_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63351</th>\n",
       "      <td>http://jhir.library.jhu.edu/handle/1774.2/836/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63352</th>\n",
       "      <td>http://jhir.library.jhu.edu/handle/1774.2/836/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63353</th>\n",
       "      <td>http://jhir.library.jhu.edu/handle/1774.2/836/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63354</th>\n",
       "      <td>http://jhir.library.jhu.edu/handle/1774.2/836/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63355</th>\n",
       "      <td>http://jhir.library.jhu.edu/handle/1774.2/836/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63356 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Link\n",
       "0      https://catalyst.library.jhu.edu/shibboleth_lo...\n",
       "1      https://catalyst.library.jhu.edu/shibboleth_lo...\n",
       "2      https://catalyst.library.jhu.edu/shibboleth_lo...\n",
       "3      https://catalyst.library.jhu.edu/shibboleth_lo...\n",
       "4      https://catalyst.library.jhu.edu/shibboleth_lo...\n",
       "...                                                  ...\n",
       "63351  http://jhir.library.jhu.edu/handle/1774.2/836/...\n",
       "63352  http://jhir.library.jhu.edu/handle/1774.2/836/...\n",
       "63353  http://jhir.library.jhu.edu/handle/1774.2/836/...\n",
       "63354  http://jhir.library.jhu.edu/handle/1774.2/836/...\n",
       "63355  http://jhir.library.jhu.edu/handle/1774.2/836/...\n",
       "\n",
       "[63356 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load crawled links from file into a dataframe (in case of termination)\n",
    "df = pd.read_csv(\"../backend/data.nosync/library-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Rename column\n",
    "df.rename(columns={0: \"Link\"}, inplace=True)\n",
    "\n",
    "# Normalize dataframe (remove any links that are not jhu.edu in base url or no https:// in the link)\n",
    "df = df[df['Link'].str.contains(\"library.jhu.edu\")]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove leading/trailing whitespace, condense multiple whitespaces to single\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def scrape_page(url, q):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'li', 'div', 'table', 'th', 'td', 'tr', 'ol', 'ul', 'blockquote', 'pre', 'code', 'caption', 'dt', 'dd']\n",
    "\n",
    "    text_data = []\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            text = clean_text(element.get_text())\n",
    "            if text:\n",
    "                text_data.append(text)\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(text_data) - 1):\n",
    "        json_data.append({\n",
    "            'prompt': text_data[i],\n",
    "            'completion': text_data[i + 1]\n",
    "        })\n",
    "        \n",
    "    # add to queue\n",
    "    q.put(json_data)\n",
    "    \n",
    "def write_to_file(q):\n",
    "    with open('../backend/data.nosync/library-jhu-edu/data.json', 'a') as f:\n",
    "        while True:\n",
    "            data = q.get()\n",
    "            if data is None:\n",
    "                break\n",
    "            json.dump(data, f)\n",
    "            f.write('\\n')\n",
    "            q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/33bb8_h50z75_0klrf43_bqm0000gn/T/ipykernel_16458/1453435867.py:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(res.text, 'html.parser')\n",
      "/Users/benjamin/anaconda3/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# List of URLs to scrape\n",
    "urls = df['Link'].tolist()\n",
    "\n",
    "# create a queue\n",
    "q = queue.Queue()\n",
    "\n",
    "# create a separate thread to write data to file\n",
    "file_writer = threading.Thread(target=write_to_file, args=(q,))\n",
    "file_writer.start()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(scrape_page, url, q) for url in urls}\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# stop the file writing thread\n",
    "q.put(None)\n",
    "file_writer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prompt/Completion pairs before cleaning: 8830924\n",
      "Number of words scraped (prompt): 221535996\n",
      "Number of words scraped (completion): 225531440\n",
      "Number of Prompt/Completion pairs after cleaning: 342748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1997 â€“ latest: SpringerLINK Contemporary 1997-...</td>\n",
       "      <td>1997 â€“ latest: SpringerLink Journals - AutoHol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Find It @ Johns Hopkins Libraries Short link J...</td>\n",
       "      <td>Find It @ Johns Hopkins Libraries Short link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Journal e &amp; i Elektrotechnik und Informationst...</td>\n",
       "      <td>Journal e &amp; i Elektrotechnik und Informationst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Journal e &amp; i Elektrotechnik und Informationst...</td>\n",
       "      <td>Journal e &amp; i Elektrotechnik und Informationst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Journal e &amp; i Elektrotechnik und Informationst...</td>\n",
       "      <td>Online Access 1997 â€“ latest: SpringerLINK Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830911</th>\n",
       "      <td>War Powers Resolution of 1973 [1]</td>\n",
       "      <td>war termination [1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830912</th>\n",
       "      <td>war termination [1]</td>\n",
       "      <td>war-fighting [1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830913</th>\n",
       "      <td>war-fighting [1]</td>\n",
       "      <td>Warburg effect [1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830922</th>\n",
       "      <td>JScholarship Home Theses and Dissertations, El...</td>\n",
       "      <td>0-9 A B C D E F G H I J K L M N O P Q R S T U ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8830923</th>\n",
       "      <td>0-9 A B C D E F G H I J K L M N O P Q R S T U ...</td>\n",
       "      <td>ascending descending 5 10 20 40 60 80 100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342748 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    prompt  \\\n",
       "115      1997 â€“ latest: SpringerLINK Contemporary 1997-...   \n",
       "123      Find It @ Johns Hopkins Libraries Short link J...   \n",
       "127      Journal e & i Elektrotechnik und Informationst...   \n",
       "128      Journal e & i Elektrotechnik und Informationst...   \n",
       "130      Journal e & i Elektrotechnik und Informationst...   \n",
       "...                                                    ...   \n",
       "8830911                  War Powers Resolution of 1973 [1]   \n",
       "8830912                                war termination [1]   \n",
       "8830913                                   war-fighting [1]   \n",
       "8830922  JScholarship Home Theses and Dissertations, El...   \n",
       "8830923  0-9 A B C D E F G H I J K L M N O P Q R S T U ...   \n",
       "\n",
       "                                                completion  \n",
       "115      1997 â€“ latest: SpringerLink Journals - AutoHol...  \n",
       "123           Find It @ Johns Hopkins Libraries Short link  \n",
       "127      Journal e & i Elektrotechnik und Informationst...  \n",
       "128      Journal e & i Elektrotechnik und Informationst...  \n",
       "130      Online Access 1997 â€“ latest: SpringerLINK Cont...  \n",
       "...                                                    ...  \n",
       "8830911                                war termination [1]  \n",
       "8830912                                   war-fighting [1]  \n",
       "8830913                                 Warburg effect [1]  \n",
       "8830922  0-9 A B C D E F G H I J K L M N O P Q R S T U ...  \n",
       "8830923          ascending descending 5 10 20 40 60 80 100  \n",
       "\n",
       "[342748 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pairs into a dataframe\n",
    "with open('../backend/data.nosync/library-jhu-edu/data.json', 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Error handling\n",
    "        try:\n",
    "            data.extend(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "pc_pairs_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs before cleaning: {len(pc_pairs_df)}\")\n",
    "print(f\"Number of words scraped (prompt): {pc_pairs_df['prompt'].str.split().str.len().sum()}\")\n",
    "print(f\"Number of words scraped (completion): {pc_pairs_df['completion'].str.split().str.len().sum()}\")\n",
    "\n",
    "# Remove any prompts/completion pairs that contains characters less than 15\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['prompt'].str.len() > 15]\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['completion'].str.len() > 15]\n",
    "\n",
    "# Remove duplicates\n",
    "pc_pairs_df.drop_duplicates(subset='prompt', keep='last', inplace=True)\n",
    "\n",
    "# Save to JSON file in the format of {\"prompt\": \"prompt text\", \"completion\": \"completion text\"}\n",
    "pc_pairs_df.to_json('../backend/data.nosync/library-jhu-edu/prompt-completion-pairs.json', orient='records', lines=True)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs after cleaning: {len(pc_pairs_df)}\")\n",
    "\n",
    "pc_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 63356\n"
     ]
    }
   ],
   "source": [
    "# Get dataset statistics\n",
    "# Amount of links (from links.txt)\n",
    "print(f\"Number of links: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
