{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(start, success_file_path, error_file_path):\n",
    "    unseen = set([start])\n",
    "    seen = set([])\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        while unseen:\n",
    "            url = unseen.pop()\n",
    "            seen.add(url)\n",
    "            executor.submit(crawl_website, url, seen, unseen, success_file_path, error_file_path)\n",
    "\n",
    "    print(f\"Finished crawling {start}\")\n",
    "    return seen \n",
    "\n",
    "def crawl_website(url, seen, unseen, success_file_path, error_file_path):\n",
    "    try:\n",
    "        new_links = getNewLinks(url, seen, unseen, success_file_path)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            for link in new_links:\n",
    "                print(f\"Adding: {link}\")\n",
    "                unseen.add(link)  \n",
    "                executor.submit(crawl_website, link, seen, unseen, success_file_path, error_file_path)\n",
    "    except:\n",
    "        print(f\"Error crawling {url}\")\n",
    "        print(f\"Error due to: {sys.exc_info()[0]}\")\n",
    "        with open(error_file_path, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "def save_links_to_file(links, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        for link in links:\n",
    "            if not is_duplicate_link(link, file_path):\n",
    "                f.write(f\"{link}\\n\")\n",
    "                \n",
    "def getNewLinks(url, seen, unseen, success_file_path):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [link['href'] for link in soup.find_all('a') if link.has_attr('href') and not any(ext in link['href'] for ext in ['.png', '.pdf', '.jpg', '.jpeg', '~json/', 'javascript:;', 'mailto:', 'webcal:'])]\n",
    "        links = [urljoin(url, link) for link in links]\n",
    "        links = [link for link in links if link not in seen and link not in unseen and urlparse(link).netloc.endswith('ventures.jhu.edu')] # Change this to limit domain\n",
    "        save_links_to_file(links, success_file_path)\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Error getting {url}\")\n",
    "        print(f\"Error due to: {response.status_code}\")\n",
    "        # Print error reason\n",
    "        print(response.reason)\n",
    "        # Print all information regarding error\n",
    "        print(response.text)\n",
    "    return []\n",
    "\n",
    "def is_duplicate_link(link, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if link.strip() == line.strip():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled_links = crawl_web('https://ventures.jhu.edu/', \"../data/ventures-jhu-edu/success_links.txt\", \"../data/ventures-jhu-edu/error_links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/33bb8_h50z75_0klrf43_bqm0000gn/T/ipykernel_21204/2515416746.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"../backend/data/ventures-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ventures.jhu.edu/#skip_content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ventures.jhu.edu/innovations/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ventures.jhu.edu/companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ventures.jhu.edu/innovations/technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ventures.jhu.edu/technology-transfer/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5825</th>\n",
       "      <td>https://ventures.jhu.edu/working-jhtv/#staff1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6295</th>\n",
       "      <td>https://ventures.jhu.edu/accessibility/#skip_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296</th>\n",
       "      <td>https://ventures.jhu.edu/accessibility/#menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>https://ventures.jhu.edu/jhtv-legal-disclaimer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>https://ventures.jhu.edu/jhtv-legal-disclaimer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3136 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Link\n",
       "0                https://ventures.jhu.edu/#skip_content\n",
       "1                 https://ventures.jhu.edu/innovations/\n",
       "2                    https://ventures.jhu.edu/companies\n",
       "3     https://ventures.jhu.edu/innovations/technolog...\n",
       "4         https://ventures.jhu.edu/technology-transfer/\n",
       "...                                                 ...\n",
       "5825      https://ventures.jhu.edu/working-jhtv/#staff1\n",
       "6295  https://ventures.jhu.edu/accessibility/#skip_c...\n",
       "6296       https://ventures.jhu.edu/accessibility/#menu\n",
       "6298  https://ventures.jhu.edu/jhtv-legal-disclaimer...\n",
       "6299  https://ventures.jhu.edu/jhtv-legal-disclaimer...\n",
       "\n",
       "[3136 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load crawled links from file into a dataframe (in case of termination)\n",
    "df = pd.read_csv(\"../backend/data/ventures-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Rename column\n",
    "df.rename(columns={0: \"Link\"}, inplace=True)\n",
    "\n",
    "# Normalize dataframe (remove any links that are not jhu.edu in base url or no https:// in the link)\n",
    "df = df[df['Link'].str.contains(\"https://ventures.jhu.edu\")]\n",
    "# Remove any links that has \"email-protection\"\n",
    "df = df[~df['Link'].str.contains(\"email-protection\")]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove leading/trailing whitespace, condense multiple whitespaces to single\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def scrape_page(url, q):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'li', 'div', 'table', 'th', 'td', 'tr', 'ol', 'ul', 'blockquote', 'pre', 'code', 'caption', 'dt', 'dd']\n",
    "\n",
    "    text_data = []\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            text = clean_text(element.get_text())\n",
    "            if text:\n",
    "                text_data.append(text)\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(text_data) - 1):\n",
    "        json_data.append({\n",
    "            'prompt': text_data[i],\n",
    "            'completion': text_data[i + 1]\n",
    "        })\n",
    "        \n",
    "    # add to queue\n",
    "    q.put(json_data)\n",
    "    \n",
    "def write_to_file(q):\n",
    "    with open('../backend/data/ventures-jhu-edu/data.json', 'a') as f:\n",
    "        while True:\n",
    "            data = q.get()\n",
    "            if data is None:\n",
    "                break\n",
    "            json.dump(data, f)\n",
    "            f.write('\\n')\n",
    "            q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of URLs to scrape\n",
    "# urls = df['Link'].tolist()\n",
    "\n",
    "# # create a queue\n",
    "# q = queue.Queue()\n",
    "\n",
    "# # create a separate thread to write data to file\n",
    "# file_writer = threading.Thread(target=write_to_file, args=(q,))\n",
    "# file_writer.start()\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#     futures = {executor.submit(scrape_page, url, q) for url in urls}\n",
    "#     concurrent.futures.wait(futures)\n",
    "\n",
    "# # stop the file writing thread\n",
    "# q.put(None)\n",
    "# file_writer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prompt/Completion pairs before cleaning: 635472\n",
      "Number of words scraped (prompt): 11600537\n",
      "Number of words scraped (completion): 11547221\n",
      "Number of Prompt/Completion pairs after cleaning: 12610\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6923</th>\n",
       "      <td>Skip to Main Content Rivier University Search....</td>\n",
       "      <td>Skip to Main Content Rivier University Search....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6951</th>\n",
       "      <td>FastForward startups are fulfilling the foundi...</td>\n",
       "      <td>FastForward startups are fulfilling the foundi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>Startups The entrepreneurs and technologies th...</td>\n",
       "      <td>Startups The entrepreneurs and technologies th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6988</th>\n",
       "      <td>startup news Watch Session 8 of JHTV's 'Emergi...</td>\n",
       "      <td>startup news Watch Session 8 of JHTV's 'Emergi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7061</th>\n",
       "      <td>Watch Session 8 of JHTV's 'Emerging CEO Panel ...</td>\n",
       "      <td>News Support JHTV Events Staff Directory Worki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635467</th>\n",
       "      <td>The Licensing Process Reports of Invention Mat...</td>\n",
       "      <td>Industry-Sponsored Research Translational Fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635468</th>\n",
       "      <td>Industry-Sponsored Research Translational Fund...</td>\n",
       "      <td>Corporate Partnerships Licensing Investments S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635469</th>\n",
       "      <td>Corporate Partnerships Licensing Investments S...</td>\n",
       "      <td>FastForward FastForward U Social Innovation La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635470</th>\n",
       "      <td>FastForward FastForward U Social Innovation La...</td>\n",
       "      <td>Faculty &amp; Inventors Students Industry Investors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635471</th>\n",
       "      <td>Faculty &amp; Inventors Students Industry Investors</td>\n",
       "      <td>In the time of COVID-19, we need every ambulan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12610 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt  \\\n",
       "6923    Skip to Main Content Rivier University Search....   \n",
       "6951    FastForward startups are fulfilling the foundi...   \n",
       "6956    Startups The entrepreneurs and technologies th...   \n",
       "6988    startup news Watch Session 8 of JHTV's 'Emergi...   \n",
       "7061    Watch Session 8 of JHTV's 'Emerging CEO Panel ...   \n",
       "...                                                   ...   \n",
       "635467  The Licensing Process Reports of Invention Mat...   \n",
       "635468  Industry-Sponsored Research Translational Fund...   \n",
       "635469  Corporate Partnerships Licensing Investments S...   \n",
       "635470  FastForward FastForward U Social Innovation La...   \n",
       "635471    Faculty & Inventors Students Industry Investors   \n",
       "\n",
       "                                               completion  \n",
       "6923    Skip to Main Content Rivier University Search....  \n",
       "6951    FastForward startups are fulfilling the foundi...  \n",
       "6956    Startups The entrepreneurs and technologies th...  \n",
       "6988    startup news Watch Session 8 of JHTV's 'Emergi...  \n",
       "7061    News Support JHTV Events Staff Directory Worki...  \n",
       "...                                                   ...  \n",
       "635467  Industry-Sponsored Research Translational Fund...  \n",
       "635468  Corporate Partnerships Licensing Investments S...  \n",
       "635469  FastForward FastForward U Social Innovation La...  \n",
       "635470    Faculty & Inventors Students Industry Investors  \n",
       "635471  In the time of COVID-19, we need every ambulan...  \n",
       "\n",
       "[12610 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pairs into a dataframe\n",
    "with open('../backend/data/ventures-jhu-edu/data.json', 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Error handling\n",
    "        try:\n",
    "            data.extend(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "pc_pairs_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs before cleaning: {len(pc_pairs_df)}\")\n",
    "print(f\"Number of words scraped (prompt): {pc_pairs_df['prompt'].str.split().str.len().sum()}\")\n",
    "print(f\"Number of words scraped (completion): {pc_pairs_df['completion'].str.split().str.len().sum()}\")\n",
    "\n",
    "# Remove any prompts/completion pairs that contains characters less than 15\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['prompt'].str.len() > 15]\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['completion'].str.len() > 15]\n",
    "\n",
    "# Remove Cloudfare blocked data\n",
    "pc_pairs_df = pc_pairs_df[~pc_pairs_df['prompt'].str.contains(\"malicious bots | Cloudflare | Your IP: | Email Protection | Performance & security | prevent this in the future | please complete the captcha below | JavaScript needs to be enabled | This process is automatic | JavaScript is required | Please stand by, while we are checking your browser | Checking your browser | DDoS protection by Cloudflare | This page is having a slideshow that uses Javascript | Please enable Javascript to view this page | This page uses Javascript | Please enable Javascript\")]\n",
    "\n",
    "# Remove duplicates\n",
    "pc_pairs_df.drop_duplicates(subset='prompt', keep='last', inplace=True)\n",
    "\n",
    "# Save to JSON file in the format of {\"prompt\": \"prompt text\", \"completion\": \"completion text\"}\n",
    "pc_pairs_df.to_json('../backend/data/ventures-jhu-edu/prompt-completion-pairs.json', orient='records', lines=True)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs after cleaning: {len(pc_pairs_df)}\")\n",
    "\n",
    "pc_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 3136\n",
      "Number of words in dataset: 1137355\n"
     ]
    }
   ],
   "source": [
    "# Get dataset statistics\n",
    "# Amount of links (from links.txt)\n",
    "print(f\"Number of links: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
