{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "import clickhouse_connect\n",
    "import openai\n",
    "import tiktoken\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Warning Suppression\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of strings ranked by relatedness to the given query\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of strings ranked by relatedness to the given query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creates Embedding Vector from Query\n",
    "    embed = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # Query for Top K Similar Cases\n",
    "    top_k = 10\n",
    "    results = client.query(f\"\"\"\n",
    "        SELECT id, text, distance(embedding, {embed}) as dist\n",
    "        FROM default.hopkins_art\n",
    "        ORDER BY dist\n",
    "        LIMIT {top_k}\n",
    "    \"\"\")\n",
    "\n",
    "    # Top K Results\n",
    "    return results.named_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    Return the number of tokens in a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to count tokens\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the string\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(query: str, model: str, token_budget: int) -> str:\n",
    "    \"\"\"\n",
    "    Return a message for GPT, with relevant source texts pulled from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "\n",
    "    Returns:\n",
    "        str: Message for GPT\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Strings Ranked by Relatedness\n",
    "    strings = strings_ranked_by_relatedness(query)\n",
    "    \n",
    "    # Prompt (TODO: Hallucinate for better prompts)\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = 'Use the below website information below to answer questions about the Johns Hopkins University. If the website information does not specify enough information, use previous knowledge to answer the question.'\n",
    "\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nJohns Hopkins article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        # Check if adding the next article will exceed the token budget\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str, model: str = GPT_MODEL, token_budget: int = 4096 - 500, print_message: bool = True,) -> str:\n",
    "    \"\"\"\n",
    "    Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        model (str): GPT model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Answer to the query\n",
    "    \"\"\"\n",
    "    message = query_message(query, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about student affairs at the Johns Hopkins University\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
