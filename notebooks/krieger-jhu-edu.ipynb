{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import queue\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_web(start, success_file_path, error_file_path):\n",
    "    unseen = set([start])\n",
    "    seen = set([])\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        while unseen:\n",
    "            url = unseen.pop()\n",
    "            seen.add(url)\n",
    "            executor.submit(crawl_website, url, seen, unseen, success_file_path, error_file_path)\n",
    "\n",
    "    print(f\"Finished crawling {start}\")\n",
    "    return seen \n",
    "\n",
    "def crawl_website(url, seen, unseen, success_file_path, error_file_path):\n",
    "    try:\n",
    "        new_links = getNewLinks(url, seen, unseen, success_file_path)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            for link in new_links:\n",
    "                print(f\"Adding: {link}\")\n",
    "                unseen.add(link)  \n",
    "                executor.submit(crawl_website, link, seen, unseen, success_file_path, error_file_path)\n",
    "    except:\n",
    "        print(f\"Error crawling {url} due to {sys.exc_info()[0]}\")\n",
    "        with open(error_file_path, 'a') as f:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "def save_links_to_file(links, file_path):\n",
    "    with open(file_path, 'a') as f:\n",
    "        for link in links:\n",
    "            if not is_duplicate_link(link, file_path):\n",
    "                f.write(f\"{link}\\n\")\n",
    "                \n",
    "def getNewLinks(url, seen, unseen, success_file_path):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        links = [link['href'] for link in soup.find_all('a') if link.has_attr('href') and not any(ext in link['href'] for ext in ['.png', '.pdf', '.jpg', '.jpeg', '~json/', 'javascript:;', 'mailto:', 'webcal:', 'email-protection'])]\n",
    "        links = [urljoin(url, link) for link in links]\n",
    "        links = [link for link in links if link not in seen and link not in unseen and urlparse(link).netloc.endswith('krieger.jhu.edu')] # Change this to limit domain\n",
    "        save_links_to_file(links, success_file_path)\n",
    "        return links\n",
    "    else:\n",
    "        print(f\"Error getting {url} with status code {response.status_code} and reason {response.reason}\")\n",
    "        print(f\"Error full response: {response.text}\")\n",
    "    return []\n",
    "\n",
    "def is_duplicate_link(link, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if link.strip() == line.strip():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def domain(url1, url2):\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawled_links = crawl_web('https://krieger.jhu.edu/', \"../backend/data.nosync/krieger-jhu-edu/success_links.txt\", \"../backend/data.nosync/krieger-jhu-edu/error_links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/33bb8_h50z75_0klrf43_bqm0000gn/T/ipykernel_9389/694360590.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"../backend/data.nosync/krieger-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://krieger.jhu.edu/#primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://krieger.jhu.edu/academics/apply/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://krieger.jhu.edu/academics/departments-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://krieger.jhu.edu/academics/majors-minors/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://krieger.jhu.edu/academics/fields/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81212</th>\n",
       "      <td>https://krieger.jhu.edu/humanities-institute/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81213</th>\n",
       "      <td>https://krieger.jhu.edu/humanities-institute/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81214</th>\n",
       "      <td>https://krieger.jhu.edu/humanities-institute/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81215</th>\n",
       "      <td>https://krieger.jhu.edu/humanities-institute/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81216</th>\n",
       "      <td>https://krieger.jhu.edu/humanities-institute/e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74502 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Link\n",
       "0                       https://krieger.jhu.edu/#primary\n",
       "1               https://krieger.jhu.edu/academics/apply/\n",
       "2      https://krieger.jhu.edu/academics/departments-...\n",
       "3       https://krieger.jhu.edu/academics/majors-minors/\n",
       "4              https://krieger.jhu.edu/academics/fields/\n",
       "...                                                  ...\n",
       "81212  https://krieger.jhu.edu/humanities-institute/e...\n",
       "81213  https://krieger.jhu.edu/humanities-institute/e...\n",
       "81214  https://krieger.jhu.edu/humanities-institute/e...\n",
       "81215  https://krieger.jhu.edu/humanities-institute/e...\n",
       "81216  https://krieger.jhu.edu/humanities-institute/e...\n",
       "\n",
       "[74502 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load crawled links from file into a dataframe (in case of termination)\n",
    "df = pd.read_csv(\"../backend/data.nosync/krieger-jhu-edu/success_links.txt\", header=None, error_bad_lines=False)\n",
    "\n",
    "# Rename column\n",
    "df.rename(columns={0: \"Link\"}, inplace=True)\n",
    "\n",
    "# Normalize dataframe (remove any links that are not jhu.edu in base url or no https:// in the link)\n",
    "df = df[df['Link'].str.contains(\"https://krieger.jhu.edu\")]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove leading/trailing whitespace, condense multiple whitespaces to single\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def scrape_page(url, q):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    tags = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'li', 'div', 'table', 'th', 'td', 'tr', 'ol', 'ul', 'blockquote', 'pre', 'code', 'caption', 'dt', 'dd']\n",
    "\n",
    "    text_data = []\n",
    "    for tag in tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            text = clean_text(element.get_text())\n",
    "            if text:\n",
    "                text_data.append(text)\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(text_data) - 1):\n",
    "        json_data.append({\n",
    "            'prompt': text_data[i],\n",
    "            'completion': text_data[i + 1]\n",
    "        })\n",
    "        \n",
    "    # add to queue\n",
    "    q.put(json_data)\n",
    "    \n",
    "def write_to_file(q):\n",
    "    with open('../backend/data.nosync/krieger-jhu-edu/data.json', 'a') as f:\n",
    "        while True:\n",
    "            data = q.get()\n",
    "            if data is None:\n",
    "                break\n",
    "            json.dump(data, f)\n",
    "            f.write('\\n')\n",
    "            q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of URLs to scrape\n",
    "urls = df['Link'].tolist()\n",
    "\n",
    "# create a queue\n",
    "q = queue.Queue()\n",
    "\n",
    "# create a separate thread to write data to file\n",
    "file_writer = threading.Thread(target=write_to_file, args=(q,))\n",
    "file_writer.start()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = {executor.submit(scrape_page, url, q) for url in urls}\n",
    "    concurrent.futures.wait(futures)\n",
    "\n",
    "# stop the file writing thread\n",
    "q.put(None)\n",
    "file_writer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Prompt/Completion pairs before cleaning: 16110435\n",
      "Number of words scraped (prompt): 231694829\n",
      "Number of words scraped (completion): 231076378\n",
      "Number of Prompt/Completion pairs after cleaning: 68428\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>Skip to content Admissions Search This Website...</td>\n",
       "      <td>Admissions Search This Website Search Departme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>Krieger School of Arts &amp; Sciences &gt; About &gt; Di...</td>\n",
       "      <td>Krieger School of Arts &amp; Sciences &gt; About &gt; Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>Johns Hopkins is a community committed to dive...</td>\n",
       "      <td>Read the Johns Hopkins Roadmap on Diversity an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>BiologyTrina Schroer[email protected]du</td>\n",
       "      <td>Biophysics Margaret Johnson [email protected]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>Johns Hopkins UniversityKrieger School of Arts...</td>\n",
       "      <td>Â© 2023 Johns Hopkins University, Zanvyl Kriege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16110428</th>\n",
       "      <td>PhD ProgramShow sub menu Requirements Admissio...</td>\n",
       "      <td>Requirements Admissions Governance Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16110429</th>\n",
       "      <td>Requirements Admissions Governance Board</td>\n",
       "      <td>No events scheduled for November 4, 2022. Jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16110432</th>\n",
       "      <td>No events scheduled for November 4, 2022. Jump...</td>\n",
       "      <td>Previous Day Next Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16110433</th>\n",
       "      <td>Previous Day Next Day</td>\n",
       "      <td>Google Calendar iCalendar Outlook 365 Outlook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16110434</th>\n",
       "      <td>Google Calendar iCalendar Outlook 365 Outlook ...</td>\n",
       "      <td>Accessibility Privacy Statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68428 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     prompt  \\\n",
       "2250      Skip to content Admissions Search This Website...   \n",
       "2258      Krieger School of Arts & Sciences > About > Di...   \n",
       "2260      Johns Hopkins is a community committed to dive...   \n",
       "2411                BiologyTrina Schroer[email protected]du   \n",
       "2534      Johns Hopkins UniversityKrieger School of Arts...   \n",
       "...                                                     ...   \n",
       "16110428  PhD ProgramShow sub menu Requirements Admissio...   \n",
       "16110429           Requirements Admissions Governance Board   \n",
       "16110432  No events scheduled for November 4, 2022. Jump...   \n",
       "16110433                              Previous Day Next Day   \n",
       "16110434  Google Calendar iCalendar Outlook 365 Outlook ...   \n",
       "\n",
       "                                                 completion  \n",
       "2250      Admissions Search This Website Search Departme...  \n",
       "2258      Krieger School of Arts & Sciences > About > Di...  \n",
       "2260      Read the Johns Hopkins Roadmap on Diversity an...  \n",
       "2411          Biophysics Margaret Johnson [email protected]  \n",
       "2534      Â© 2023 Johns Hopkins University, Zanvyl Kriege...  \n",
       "...                                                     ...  \n",
       "16110428           Requirements Admissions Governance Board  \n",
       "16110429  No events scheduled for November 4, 2022. Jump...  \n",
       "16110432                              Previous Day Next Day  \n",
       "16110433  Google Calendar iCalendar Outlook 365 Outlook ...  \n",
       "16110434                    Accessibility Privacy Statement  \n",
       "\n",
       "[68428 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pairs into a dataframe\n",
    "with open('../backend/data.nosync/krieger-jhu-edu/data.json', 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        # Error handling\n",
    "        try:\n",
    "            data.extend(json.loads(line))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "pc_pairs_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs before cleaning: {len(pc_pairs_df)}\")\n",
    "print(f\"Number of words scraped (prompt): {pc_pairs_df['prompt'].str.split().str.len().sum()}\")\n",
    "print(f\"Number of words scraped (completion): {pc_pairs_df['completion'].str.split().str.len().sum()}\")\n",
    "\n",
    "# Remove any prompts/completion pairs that contains characters less than 15\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['prompt'].str.len() > 15]\n",
    "pc_pairs_df = pc_pairs_df[pc_pairs_df['completion'].str.len() > 15]\n",
    "\n",
    "# Remove duplicates\n",
    "pc_pairs_df.drop_duplicates(subset='prompt', keep='last', inplace=True)\n",
    "\n",
    "# Save to JSON file in the format of {\"prompt\": \"prompt text\", \"completion\": \"completion text\"}\n",
    "pc_pairs_df.to_json('../backend/data.nosync/krieger-jhu-edu/prompt-completion-pairs.json', orient='records', lines=True)\n",
    "\n",
    "print(f\"Number of Prompt/Completion pairs after cleaning: {len(pc_pairs_df)}\")\n",
    "\n",
    "pc_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 74502\n"
     ]
    }
   ],
   "source": [
    "# Get dataset statistics\n",
    "# Amount of links (from links.txt)\n",
    "print(f\"Number of links: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
