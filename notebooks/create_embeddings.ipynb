{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "import clickhouse_connect\n",
    "import openai\n",
    "import tiktoken\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Warning Suppression\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"unclosed\", category=ResourceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "BATCH_SIZE = 2000\n",
    "characters_per_index = 32750 // 16 # Characters per API Limit\n",
    "\n",
    "# Clickhouse Connection\n",
    "client = clickhouse_connect.get_client(\n",
    "      host='msc-37984436.us-east-1.aws.myscale.com',\n",
    "      port=8443,\n",
    "      username='lcai99',\n",
    "      password='passwd_GTDU9YGWNkp9VV'\n",
    ")\n",
    "\n",
    "# Keys\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# File Paths\n",
    "filename = 'prompt-completion-pairs-combined.json' # File containing prompt-completion pairs for training\n",
    "SAVE_PATH = \"combinedeigth.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(string, chunk_size) -> list:\n",
    "    \"\"\"\n",
    "    Splits a string into chunks of size chunk_size\n",
    "\n",
    "    Args:\n",
    "        string (str): String to split\n",
    "        chunk_size (int): Size of each chunk\n",
    "\n",
    "    Returns:\n",
    "        list: List of chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    return [string[i:i+chunk_size] for i in range(0, len(string), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_string_list(filename, characters_per_index) -> list:\n",
    "    \"\"\"\n",
    "    Creates a list of strings from a file\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of file to read\n",
    "        characters_per_index (int): Number of characters per string\n",
    "\n",
    "    Returns:\n",
    "        list: List of strings\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    string_list = split_string(content, characters_per_index)\n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T19:54:23.179895Z",
     "start_time": "2023-06-21T19:54:22.273165Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = create_string_list(filename, characters_per_index)\n",
    "# print(len(data))\n",
    "\n",
    "# Split Data into Segments\n",
    "segmented_data = [data[i * len(data) // 15: (i + 1) * len(data) // 15] for i in range(15)]\n",
    "segmented_data[-1] += data[14 * len(data) // 15:]  # Ensure last segment contains all remaining data\n",
    "\n",
    "# Store Segments Into Data\n",
    "data = segmented_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-21T19:57:37.424691Z",
     "start_time": "2023-06-21T19:54:30.048991Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Embeddings\n",
    "embeddings = []\n",
    "\n",
    "for batch_start in range(0, len(data), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = data[batch_start:batch_end]\n",
    "    \n",
    "    # Progress Indicator\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    \n",
    "    # Create Embeddings using OpenAI API\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    \n",
    "    # Ensure Embeddings are in Same Order as Input\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]\n",
    "        \n",
    "    # Extract Embeddings and Append to List\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Dataframe of Embeddings\n",
    "df = pd.DataFrame({\"text\": data, \"embedding\": embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Save Embeddings to CSV\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T03:11:56.886093Z",
     "start_time": "2023-06-22T03:11:47.217691Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Embeddings from CSV to Dataframe\n",
    "df = pd.read_csv(\"combinedeigth.csv\")\n",
    "df['embedding'] = df['embedding'].str.strip('[]').str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T03:18:10.068384Z",
     "start_time": "2023-06-22T03:17:11.172881Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Store Embeddings in ClickHouse ###\n",
    "embedding_len = len(df['embedding'][0]) # 1536\n",
    "\n",
    "# ClickHouse SQL Query to Create Table\n",
    "client.command(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.hopkins_art\n",
    "(\n",
    "    id        UInt32,\n",
    "    text      String,\n",
    "    embedding Array(Float32),\n",
    "    CONSTRAINT cons_embedding_len CHECK length(embedding) = {embedding_len},\n",
    "    VECTOR INDEX article_content_index embedding TYPE HNSWFLAT('metric_type=Cosine')\n",
    ")\n",
    "ENGINE = MergeTree ORDER BY id\n",
    "\"\"\")\n",
    "\n",
    "# Insert Embeddings into ClickHouse in Batches\n",
    "batch_size = 100\n",
    "total_records = len(df)\n",
    "\n",
    "# Convert Embeddings to List of Lists\n",
    "data = df.to_records(index=False).tolist()\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Batch Insertion\n",
    "for i in tqdm(range(21900, total_records, batch_size)):\n",
    "    i_end = min(i + batch_size, total_records)\n",
    "    client.insert(\"default.hopkins_art\", data[i:i_end], column_names=column_names)\n",
    "    \n",
    "# Verify\n",
    "print(f\"articles count: {client.command('SELECT count(*) FROM default.hopkins_art')}\") # Check Count of Data\n",
    "get_index_status=\"SELECT status FROM system.vector_indices WHERE name='article_content_index'\"\n",
    "print(f\"index build status: {client.command(get_index_status)}\") # Check Index Status (Vector Index is Built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T02:39:46.503904Z",
     "start_time": "2023-06-22T02:39:46.485593Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of strings ranked by relatedness to the given query\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "\n",
    "    Returns:\n",
    "        list[str]: List of strings ranked by relatedness to the given query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creates Embedding Vector from Query\n",
    "    embed = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\",\n",
    "    )[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # Query for Top K Similar Cases\n",
    "    top_k = 10\n",
    "    results = client.query(f\"\"\"\n",
    "        SELECT id, text, distance(embedding, {embed}) as dist\n",
    "        FROM default.hopkins_art\n",
    "        ORDER BY dist\n",
    "        LIMIT {top_k}\n",
    "    \"\"\")\n",
    "\n",
    "    # Top K Results\n",
    "    return results.named_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    Return the number of tokens in a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to count tokens\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the string\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(query: str, model: str, token_budget: int) -> str:\n",
    "    \"\"\"\n",
    "    Return a message for GPT, with relevant source texts pulled from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "\n",
    "    Returns:\n",
    "        str: Message for GPT\n",
    "    \"\"\"\n",
    "\n",
    "    # Get Strings Ranked by Relatedness\n",
    "    strings = strings_ranked_by_relatedness(query)\n",
    "    \n",
    "    # Prompt (TODO: Hallucinate for better prompts)\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = 'Use the below website information below to answer questions about the Johns Hopkins University. If the website information does not specify enough information, use previous knowledge to answer the question.'\n",
    "\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nJohns Hopkins article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        # Check if adding the next article will exceed the token budget\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T02:39:47.257512Z",
     "start_time": "2023-06-22T02:39:47.254568Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ask(query: str, model: str = GPT_MODEL, token_budget: int = 4096 - 500, print_message: bool = True,) -> str:\n",
    "    \"\"\"\n",
    "    Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        model (str): GPT model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Answer to the query\n",
    "    \"\"\"\n",
    "    message = query_message(query, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about student affairs at the Johns Hopkins University\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
